{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Extracting Named Entities using spaCy (NLP software) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Created a temporary list of dictionaries to mimic the data that was collected. \n",
    "1. Did all testing of the spaCy library with named entity extraction on this temporary dictionary. \n",
    "2. These entries are copy and pasted directly from the RSS feed, so that I can have a accurate picture of how my code works. \n",
    "3. You can see some deficiencies in spaCy, but there are named entities that I would like to include in further data preparation such as:\n",
    "    1. NORP \n",
    "    2. DATE \n",
    "    3. PERSON and PRONOUN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_mentioned_temp = [{'title':\"Julio Tello, Peru’s Archaeological Trail Blazer\", \n",
    "                           'description_clean':'Tello is often called some variation of' \n",
    "                           'the father of Peruvian archaeology or the first indigenous Peruvian archaeologist.' \n",
    "                           'And his work was playing out across a backdrop of constant unrest and conflict,' \n",
    "                           'both for his country and his profession'},\n",
    "                          {'title':'Grand Central Terminal', 'description_clean': 'Grand Central\"s story starts with' \n",
    "                           'one of the wealthiest names in U.S. history,' \n",
    "                           ' but it also is in many ways the story of the city itself since the 1800s,' \n",
    "                           'because Grand Central was such a pivotal element in the growth of Manhattan.'},\n",
    "                          {'title':'A Brief History of Bonsai', 'description_clean':'Bonsai’s origins go all the way back' \n",
    "                           'to ancient China,long before Japan became infatuated with the art form.' \n",
    "                           ' Over time, the western world also became fascinated with bonsai,' \n",
    "                           'though there has been plenty of cultural confusion about it along the way.'},\n",
    "                          {'title':'The Peterloo Massacre', 'description_clean': 'The Peterloo Massacre took place' \n",
    "                           'during a peaceful protest for parliamentary reform in Manchester, England.'\n",
    "                           ' And there was a lot feeding into why people in Britain,' \n",
    "                           'and specifically in the region around Manchester, thought that reform was needed.'},\n",
    "                          {'title':'The Nika Riots & Massacre', 'description_clean':'Large-scale rioting and mass violence' \n",
    "                           'were fairly common in Constantinople when this riot – and then massacre – took place in the year 532.' \n",
    "                           ' But we have more documentation of the Nika Riots than many of the others.'},\n",
    "                          {'title':\"Lakshmi Bai: Who is India's Joan of Arc?\",'description_clean':'Lakshmi Bai was born into' \n",
    "                           'wealthy family in 1830, but she was far from the typical aristocrat.'\n",
    "                           'In this episode, Deblina and Sarah recount the life and work of Lakshmi Bai,' \n",
    "                           'from her youth to her instrumental role in the Indian Rebellion of 1857.'},\n",
    "                          {'title':'The First Tacoma Narrows Bridge – Galloping Gertie', 'description_clean': \n",
    "                           'The drama of the first Tacoma Narrows bridge is hardly relegated to its turbulent end.'\n",
    "                           'There’s more to the story – from its inception to financing issues'\n",
    "                           'to some surprising legal happenings, and how it spawned entirely new approaches to bridge design.'},\n",
    "                          {'title':'A. Gustave Eiffel, Part 2', 'description_clean': 'The second part of our look at Gustave'\n",
    "                           'Eiffel\"s life picks up just after he closed down all business interests in South America,'\n",
    "                           'and leads into some of his most famous work, including the Statue of Liberty '\n",
    "                           'and the Parisian tower that bears his name.'},\n",
    "                          {'title':'The Dyatlov Pass Incident', 'description_clean':'In 1959, nine students ventured into'\n",
    "                           'the Ural mountains for a ski hiking trip, and never returned. While much speculation has'\n",
    "                           'swirled for more than half a century, no one knows for certain what caused them to abandon'\n",
    "                           'their camp to die in the cold.'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Country, Cities, States (GPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function extracting geopolitical entities. \n",
    "def spacy_gpe(texts):\n",
    "    \n",
    "    #Because it's a list of dictionaries and multiple texts, this doc=nlp(texts) and looping through it to work around a limitation of spaCy. \n",
    "    doc = nlp(texts)\n",
    "    \n",
    "    #List of results of the for loop. \n",
    "    ep_results = []\n",
    "    \n",
    "    #Loop through the titles and descriptions looking for geopolitical entities or GPE\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='GPE' and ent.text=='Deblina': \n",
    "            continue \n",
    "        elif ent.label_=='GPE':\n",
    "            ep_results.append([ent.text,ent.label_])\n",
    "        \n",
    "    return ep_results\n",
    "\n",
    "#Create a list to put the results of search and extraction. \n",
    "list_titles_descriptions = []\n",
    "\n",
    "#this for loop made sure that title and description were linked together. \n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['title_gpe'] = spacy_gpe(ep['title'])\n",
    "    spacy_result['description_gpe'] = spacy_gpe(ep['description_clean'])\n",
    "    \n",
    "    #append results to the list before loop. \n",
    "    list_titles_descriptions.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title_gpe': [['Peru', 'GPE']], 'description_gpe': []},\n",
       " {'title_gpe': [], 'description_gpe': [['U.S.', 'GPE'], ['Manhattan', 'GPE']]},\n",
       " {'title_gpe': [], 'description_gpe': [['China', 'GPE'], ['Japan', 'GPE']]},\n",
       " {'title_gpe': [],\n",
       "  'description_gpe': [['Manchester', 'GPE'],\n",
       "   ['England', 'GPE'],\n",
       "   ['Britain', 'GPE']]},\n",
       " {'title_gpe': [], 'description_gpe': [['Constantinople', 'GPE']]},\n",
       " {'title_gpe': [['India', 'GPE']], 'description_gpe': []},\n",
       " {'title_gpe': [], 'description_gpe': []},\n",
       " {'title_gpe': [], 'description_gpe': []},\n",
       " {'title_gpe': [], 'description_gpe': []}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_titles_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Nationalities, religious or political groups (NORP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code does the same things as above, but it just looks for nationalities, religious or political groups. \n",
    "# This is an entity I would like to include to further prepare the data and make it more accurate. \n",
    "def spacy_norp(texts):\n",
    "    doc = nlp(texts)\n",
    "    ep_results = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='NORP': \n",
    "            ep_results.append([ent.text,ent.label_])\n",
    "        \n",
    "    return ep_results\n",
    "\n",
    "list_norp_mentioned = []\n",
    "\n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['norp_title'] = spacy_norp(ep['title'])\n",
    "    spacy_result['norp_description'] = spacy_norp(ep['description_clean'])\n",
    "    \n",
    "    list_norp_mentioned.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'norp_title': [],\n",
       "  'norp_description': [['Peruvian', 'NORP'], ['Peruvian', 'NORP']]},\n",
       " {'norp_title': [], 'norp_description': []},\n",
       " {'norp_title': [], 'norp_description': []},\n",
       " {'norp_title': [], 'norp_description': []},\n",
       " {'norp_title': [], 'norp_description': []},\n",
       " {'norp_title': [], 'norp_description': []},\n",
       " {'norp_title': [], 'norp_description': []},\n",
       " {'norp_title': [], 'norp_description': [['Parisian', 'NORP']]},\n",
       " {'norp_title': [], 'norp_description': []}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see, it's not perfect. It didn't pick up 'Indian' as a nationality and perhaps is thinking based on context that it's an event. \n",
    "list_norp_mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting buidlings, airports, highways, bridges, etc.(FAC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_fac(texts):\n",
    "    doc = nlp(texts)\n",
    "    ep_results = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='FAC': \n",
    "            ep_results.append([ent.text,ent.label_])\n",
    "        \n",
    "    return ep_results\n",
    "\n",
    "list_fac_mentioned = []\n",
    "\n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['fac_title'] = spacy_fac(ep['title'])\n",
    "    spacy_result['fac_description'] = spacy_fac(ep['description_clean'])\n",
    "    \n",
    "    list_fac_mentioned.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fac_title': [['Archaeological Trail Blazer', 'FAC']],\n",
       "  'fac_description': []},\n",
       " {'fac_title': [], 'fac_description': []},\n",
       " {'fac_title': [], 'fac_description': []},\n",
       " {'fac_title': [], 'fac_description': []},\n",
       " {'fac_title': [], 'fac_description': []},\n",
       " {'fac_title': [], 'fac_description': []},\n",
       " {'fac_title': [], 'fac_description': [['Tacoma Narrows', 'FAC']]},\n",
       " {'fac_title': [], 'fac_description': [['the Statue of Liberty', 'FAC']]},\n",
       " {'fac_title': [], 'fac_description': []}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_fac_mentioned\n",
    "#My opinion is that this named entity seems rather limited compared to the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting non-geopolitical locations like mountain ranges, bodies of water (LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_loc(texts):\n",
    "    doc = nlp(texts)\n",
    "    ep_results = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='LOC': \n",
    "            ep_results.append([ent.text,ent.label_])\n",
    "        \n",
    "    return ep_results\n",
    "\n",
    "list_loc_mentioned =[]\n",
    "\n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['loc_title'] = spacy_loc(ep['title'])\n",
    "    spacy_result['loc_description'] = spacy_loc(ep['description_clean'])\n",
    "    \n",
    "    list_loc_mentioned.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loc_title': [], 'loc_description': []},\n",
       " {'loc_title': [], 'loc_description': []},\n",
       " {'loc_title': [], 'loc_description': []},\n",
       " {'loc_title': [], 'loc_description': [['Manchester', 'LOC']]},\n",
       " {'loc_title': [], 'loc_description': []},\n",
       " {'loc_title': [], 'loc_description': [['the Indian Rebellion', 'LOC']]},\n",
       " {'loc_title': [], 'loc_description': []},\n",
       " {'loc_title': [], 'loc_description': [['South America', 'LOC']]},\n",
       " {'loc_title': [], 'loc_description': []}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_loc_mentioned\n",
    "#doesn't seem to be working very well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_event(texts):\n",
    "    doc = nlp(texts)\n",
    "    ep_results = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='EVENT': \n",
    "            ep_results.append([ent.text,ent.label_])\n",
    "        \n",
    "    return ep_results\n",
    "\n",
    "list_event_mentioned = []\n",
    "\n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['event_title'] = spacy_event(ep['title'])\n",
    "    spacy_result['event_description'] = spacy_event(ep['description_clean'])\n",
    "    \n",
    "    list_event_mentioned.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event_title': [], 'event_description': []},\n",
       " {'event_title': [], 'event_description': []},\n",
       " {'event_title': [], 'event_description': []},\n",
       " {'event_title': [], 'event_description': []},\n",
       " {'event_title': [], 'event_description': []},\n",
       " {'event_title': [], 'event_description': []},\n",
       " {'event_title': [['Tacoma Narrows Bridge', 'EVENT']],\n",
       "  'event_description': []},\n",
       " {'event_title': [], 'event_description': []},\n",
       " {'event_title': [], 'event_description': []}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_event_mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting time (DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_time(texts):\n",
    "    doc = nlp(texts)\n",
    "    ep_results = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='DATE':\n",
    "            ep_results.append([ent.text,ent.label_])\n",
    "        \n",
    "    return ep_results\n",
    "\n",
    "list_time_mentioned = []\n",
    "\n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['time_mentioned'] = spacy_time(ep['description_clean'])\n",
    "    \n",
    "    list_time_mentioned.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'time_mentioned': []},\n",
       " {'time_mentioned': [['the 1800s', 'DATE']]},\n",
       " {'time_mentioned': []},\n",
       " {'time_mentioned': []},\n",
       " {'time_mentioned': [['the year 532', 'DATE']]},\n",
       " {'time_mentioned': [['1830', 'DATE'], ['1857', 'DATE']]},\n",
       " {'time_mentioned': []},\n",
       " {'time_mentioned': []},\n",
       " {'time_mentioned': [['1959', 'DATE']]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_time_mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Persons Mentioned (PERSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_person(texts):\n",
    "    doc = nlp(texts) \n",
    "    ep_results = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='PERSON' and (ent.text=='Sarah' or ent.text=='Katie' or ent.text=='Holly' or ent.text=='Tracy' or ent.text=='Deblina' or ent.text=='Josh' or ent.text=='Candace'):\n",
    "            continue \n",
    "    #I don't want it to extract hosts names. I'm not sure how to write this. \n",
    "    #also, tricky because some of the hosts have names that are identical to episdoe topics \n",
    "        elif ent.label_=='PERSON':\n",
    "            ep_results.append([ent.text,ent.label_])\n",
    "    return ep_results\n",
    "\n",
    "list_person_mentioned = []\n",
    "\n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['title_person'] = spacy_person(ep['title'])\n",
    "    spacy_result['description_person'] = spacy_person(ep['description_clean'])\n",
    "    \n",
    "    list_person_mentioned.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title_person': [['Julio Tello', 'PERSON']], 'description_person': []},\n",
       " {'title_person': [], 'description_person': []},\n",
       " {'title_person': [], 'description_person': [['Bonsai', 'PERSON']]},\n",
       " {'title_person': [], 'description_person': []},\n",
       " {'title_person': [], 'description_person': []},\n",
       " {'title_person': [['Lakshmi Bai', 'PERSON'], ['Joan of Arc', 'PERSON']],\n",
       "  'description_person': [['Lakshmi Bai', 'PERSON'],\n",
       "   ['Lakshmi Bai', 'PERSON']]},\n",
       " {'title_person': [], 'description_person': []},\n",
       " {'title_person': [['A. Gustave Eiffel', 'PERSON']], 'description_person': []},\n",
       " {'title_person': [], 'description_person': []}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_person_mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Pronouns for Male/Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pronouns(texts):\n",
    "    doc = nlp(texts)\n",
    "    ep_results = []\n",
    "    for token in doc:\n",
    "        if token.text=='we' or token.text=='We': #excluding 'we' because this usually refers to the hosts. \n",
    "            continue \n",
    "        elif token.pos_=='PRON' and token.tag_=='PRP':\n",
    "            #token.tag_ 'PRP' means that it's a personal pronoun.\n",
    "            ep_results.append([token.text, token.pos_,token.tag_, token.dep_])\n",
    "        \n",
    "    return ep_results\n",
    "\n",
    "list_pronouns_mentioned = []\n",
    "\n",
    "for ep in country_mentioned_temp:\n",
    "    spacy_result = {}\n",
    "    spacy_result['pronouns'] = spacy_pronouns(ep['description_clean'])\n",
    "    \n",
    "    list_pronouns_mentioned.append(spacy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pronouns': []},\n",
       " {'pronouns': [['it', 'PRON', 'PRP', 'nsubj'],\n",
       "   ['itself', 'PRON', 'PRP', 'appos']]},\n",
       " {'pronouns': [['it', 'PRON', 'PRP', 'pobj']]},\n",
       " {'pronouns': []},\n",
       " {'pronouns': []},\n",
       " {'pronouns': [['she', 'PRON', 'PRP', 'nsubj']]},\n",
       " {'pronouns': [['it', 'PRON', 'PRP', 'nsubj']]},\n",
       " {'pronouns': [['he', 'PRON', 'PRP', 'nsubj']]},\n",
       " {'pronouns': [['them', 'PRON', 'PRP', 'nsubj']]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pronouns_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
